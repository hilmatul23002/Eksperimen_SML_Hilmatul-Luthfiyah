# -*- coding: utf-8 -*-
"""automate_DataSekolah

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bJGppDdUm7-npOgcX6gS6O4-OO8bNPzR

# **1. Perkenalan Dataset**

Tahap pertama, Anda harus mencari dan menggunakan dataset dengan ketentuan sebagai berikut:

1. **Sumber Dataset**:  
   Dataset dapat diperoleh dari berbagai sumber, seperti public repositories (*Kaggle*, *UCI ML Repository*, *Open Data*) atau data primer yang Anda kumpulkan sendiri.

# **2. Import Library**

Pada tahap ini, Anda perlu mengimpor beberapa pustaka (library) Python yang dibutuhkan untuk analisis data dan pembangunan model machine learning atau deep learning.
"""

!pip install kaggle pandas

!pip install feature-engine

import pandas as pd
import os
import kagglehub
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder
from feature_engine.outliers import Winsorizer
from sklearn.compose import ColumnTransformer

"""# **3. Memuat Dataset**

Pada tahap ini, Anda perlu memuat dataset ke dalam notebook. Jika dataset dalam format CSV, Anda bisa menggunakan pustaka pandas untuk membacanya. Pastikan untuk mengecek beberapa baris awal dataset untuk memahami strukturnya dan memastikan data telah dimuat dengan benar.

Jika dataset berada di Google Drive, pastikan Anda menghubungkan Google Drive ke Colab terlebih dahulu. Setelah dataset berhasil dimuat, langkah berikutnya adalah memeriksa kesesuaian data dan siap untuk dianalisis lebih lanjut.

Jika dataset berupa unstructured data, silakan sesuaikan dengan format seperti kelas Machine Learning Pengembangan atau Machine Learning Terapan
"""

# Mengunduh dataset dari kaggle
dataset_path = kagglehub.dataset_download('puanbeningpastika/dataset-pendidikan-sd-indonesia-2023-2024')
print("Dataset berhasil diunduh di folder:", dataset_path)

files = os.listdir(dataset_path)
file_path = os.path.join(dataset_path, files[0])

df = pd.read_csv(file_path)
# Drop the 'Unnamed: 14' column as it appears to contain only NaN values
if 'Unnamed: 14' in df.columns:
    df = df.drop(columns=['Unnamed: 14'])

csv_output_path = 'Data-Sekolah.csv'
df.to_csv(csv_output_path, index=False)
print("CSV berhasil disimpan di:", csv_output_path)

df.head()

"""# **5. Data Preprocessing**

Pada tahap ini, data preprocessing adalah langkah penting untuk memastikan kualitas data sebelum digunakan dalam model machine learning.

Jika Anda menggunakan data teks, data mentah sering kali mengandung nilai kosong, duplikasi, atau rentang nilai yang tidak konsisten, yang dapat memengaruhi kinerja model. Oleh karena itu, proses ini bertujuan untuk membersihkan dan mempersiapkan data agar analisis berjalan optimal.

Berikut adalah tahapan-tahapan yang bisa dilakukan, tetapi **tidak terbatas** pada:
1. Menghapus atau Menangani Data Kosong (Missing Values)
2. Menghapus Data Duplikat
3. Normalisasi atau Standarisasi Fitur
4. Deteksi dan Penanganan Outlier
5. Encoding Data Kategorikal
6. Binning (Pengelompokan Data)

Cukup sesuaikan dengan karakteristik data yang kamu gunakan yah. Khususnya ketika kami menggunakan data tidak terstruktur.
"""

df = df.dropna()

def preprocess_data(df):
    df = df.drop_duplicates()
    return df

# Define 'cols' with the numerical columns to be scaled.
# It is crucial to ensure that the DataFrame 'df' is not empty before this step.
# The 'Unnamed: 14' column should ideally be dropped earlier if it causes df to become empty.

# Select all numerical columns, excluding 'Provinsi' as it's categorical.
# If 'Unnamed: 14' was not dropped, it might be included here but will be handled if df is empty.
# Assuming 'Provinsi' is of object/string type and other relevant columns are numerical.
numerical_cols_to_scale = df.select_dtypes(include=['number']).columns.tolist()

# Ensure 'Provinsi' is not in the list if it accidentally was, or if its type changed.
if 'Siswa' in numerical_cols_to_scale:
    numerical_cols_to_scale.remove('Siswa')

cols = numerical_cols_to_scale

# Proceed with scaling only if the DataFrame is not empty and there are columns to scale
if not df.empty and cols:
    scaler = MinMaxScaler()
    df[cols] = scaler.fit_transform(df[cols])
    print("Selected numerical columns have been scaled using MinMaxScaler.")
elif df.empty:
    print("Error: The DataFrame is empty. Please ensure 'Unnamed: 14' is dropped and df.dropna() doesn't remove all rows before this step.")
else:
    print("No numerical columns found to scale.")

winsor = Winsorizer(
    capping_method='iqr',
    tail='both',
    fold=1.5,
    # The column 'Pendapatan' does not exist in the dataframe.
    # Please replace with a valid numerical column you wish to cap for outliers.
    # For example, variables=['Siswa'] or variables=['Mengulang']
    # variables=['Pendapatan']
)

# df = winsor.fit_transform(df)

df_encoded = pd.get_dummies(
    df,
    columns=['Provinsi'],
    drop_first=True
)

def preprocess_data(df):

    num_cols = df.select_dtypes(include=['int64', 'float64']).columns
    cat_cols = df.select_dtypes(include=['object', 'category']).columns

    preprocessor = ColumnTransformer([
        ('num', StandardScaler(), num_cols),
        ('cat', OneHotEncoder(drop='first'), cat_cols)
    ])

    X = preprocessor.fit_transform(df)

    return X

import os

X = preprocess_data(df)

# Ensure the directory exists before saving the file
os.makedirs('data/processed', exist_ok=True)

# simpan hasil
pd.DataFrame(X).to_csv(
    'data/processed/dataset_processed.csv',
    index=False
)

print("Preprocessing selesai.")
